# -*- coding: utf-8 -*-
"""LVADSUSR166_Final_Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gzKp_QXaeJzTX-gA82TWV1_-0mXUbBHL
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.stats import zscore
from scipy import stats
import warnings
warnings.filterwarnings("ignore")

# 1 Loading the dataset
df=pd.read_csv(r'/content/customer_segmentation.csv')

# 2 Preprocessing
# 3 EDA
# both start here
df.info()

df.head(20)

df.isnull().sum()

# Handling Missing Values
df['Income'].fillna(df['Income'].median(), inplace=True)

df.duplicated().sum()

# Since there are no duplicate values we don't need to work on removing them

df.describe()

df.describe(include = "all")

df.shape

label_encoder = LabelEncoder()
categorical_cols = df.select_dtypes(include=['object']).columns
for col in categorical_cols:
    df[col] = label_encoder.fit_transform(df[col])

# Visualizing distributions
for col in df.columns:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

# Outlier detection and treatment
for col in df.columns:
    if df[col].dtype in ['float64', 'int64']:
        plt.figure(figsize=(8, 4))
        sns.boxplot(x=df[col])
        plt.title(f'Boxplot of {col}')
        plt.show()
        z_scores = np.abs(stats.zscore(df[col]))
        threshold = 3
        outliers = np.where(z_scores > threshold)[0]
        print(f'Outliers in {col}: {outliers}')
        df[col].iloc[outliers] = df[col].mean()

plt.figure(figsize=(20, 8))
sns.heatmap(df.corr(), annot=True, cmap='viridis', fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

df.corr()

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)

# 4 Model Training
wcss = []

for i in range(1,11):
  kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
  kmeans.fit(scaled_data)

  wcss.append(kmeans.inertia_)

sns.set()
plt.plot(range(1,11), wcss)
plt.title('The Elbow Point Graph')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

num_clusters = 2
kmeans = KMeans(n_clusters=num_clusters, init='k-means++', n_init=10, max_iter=200, tol=0.0001, random_state=42)
Y=kmeans.fit_predict(scaled_data)
labels = kmeans.labels_
centroids = kmeans.cluster_centers_

plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=labels, s=50, cmap="viridis", alpha=0.8)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='o', s=110, label="Centroids")
plt.title("K-Means Clustering")
plt.legend()
plt.show()

print("Cluster Centers:")
cluster_centers = scaler.inverse_transform(centroids)
for i, center in enumerate(cluster_centers):
    print(f"Cluster {i+1}:")
    for j, val in enumerate(center):
        print(f"\t{df.columns[j]}: {val}")

# 5 Evaluation Metrics
silhouette_score(scaled_data,kmeans.fit_predict(scaled_data))